## PKINGS Terra Processing

## December 2023

## Copy Data to Google Cloud

All files stored on Amazon S3 need to be transferred to Google Cloud via HPCC.

1. Setup Terra.bio Workspace and obtain bucket for work
2. Setup .env file with the various required parameters
3. Run the following:

source pkings_gwas.env
bash code/01_Terra/01_run_upload.sh input_data/01_Terra/2023_file_payload.txt

## Configure Reference Data:

### Create Reference

The run for this reference is the new Paramromyrops kingsleyae _chromosome level_ assembly MIC4273_HAP1_MSU618.fa

### Create Interval files

#### setup virtual machines

source pkings_gwas.env
docker run -d -it --name gatk --mount type=bind,source=$root/input_data/00_Reference_Genome/,target=/input_data --mount type=bind,source=$root/input_data/01_Terra/,target=/output_data broadinstitute/gatk

#### prepare files

docker exec gatk cat /input_data/$reference.fai | cut -f1 > $root/input_data/01_Terra/intervals.list

docker exec gatk gatk CreateSequenceDictionary R= /input_data/$reference

docker exec gatk ./gatk SplitIntervals -R /input_data/$reference -L /output_data/intervals.list -scatter 50 -O /output_data/interval-files -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW

docker exec gatk cat /input_data/$reference.fai | cut -f1,2 | sort -nrk 2 | head -25 | cut -f1 > $root/input_data/01_Terra/chrom_intervals.list
docker exec gatk cat /input_data/$reference.fai | cut -f1,2 | sort -nrk 2 | tail +26 | cut -f1 > $root/input_data/01_Terra/remaining_intervals.list

docker exec gatk ./gatk SplitIntervals -R /input_data/$reference -L /output_data/chrom_intervals.list --scatter-count 100 -O /output_data/chrom-interval-files --subdivision-mode INTERVAL_SUBDIVISION

docker exec gatk ./gatk SplitIntervals -R /input_data/$reference -L /output_data/remaining_intervals.list --scatter-count 100 -O /output_data/remaining-interval-files --subdivision-mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW

cd $root/input_data/01_Terra

ls -1 ./interval-files > ./ifiles.txt

ls -1 ./chrom-interval-files > ./uifiles1.txt
ls -1 ./remaining-interval-files > ./uifiles2.txt

awk -v bucket=$bucket '{ print "gs://"bucket"/interval-files/" $0}' ./ifiles.txt > ./scattered-intervals-file.txt

awk -v bucket=$bucket '{ print "gs://"bucket"/chrom-interval-files/" $0}' ./uifiles1.txt >./unpadded_intervals_file.txt
awk -v bucket=$bucket '{ print "gs://"bucket"/remaining-interval-files/" $0}' ./uifiles2.txt >> ./unpadded_intervals_file.txt

rm ./ifiles.txt
rm ./uifiles1.txt
rm ./uifiles2.txt

#### Upload to Workspace Bucket

1. Transfer `./unpadded_intervals_file.txt`, `./scattered-intervals-file.txt` and the directories `interval-files` and `unpadded-interval-files` to the Google Bucket

   cd input_data/01_Terra
   module purge
   conda activate gcloud

   gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -n ./unpadded_intervals_file.txt gs://${bucket}/
    gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -n ./scattered-intervals-file.txt gs://${bucket}/
   gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -r -n ./interval-files gs://${bucket}/
    gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -r -n ./chrom-interval-files gs://${bucket}/

## Setup Workspace

1. Upload `.input_data/01_Terra/data_model/fish_data_2023.txt`to Terra
2. Download Airtable `input_data/01_Terra/data_model/Deduplicated Inventory-GWAS_Set.csv`
3. Run
   python3 convert_airtable_to_terra.py
4. Upload `./data_model/samples_2023.tsv` generated by the above python script
5. Generate Reference Index (BWA):
   bwa index input_data/00_Reference_Genome/${reference}
6. Transfer `input_data/00_Reference_Genome` to Terra
   conda activate gcloud
   source pkings_gwas.env
   gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -n -r ./input_data/00_Reference_Genome gs://${bucket}/reference

7. Upload `./data_model/pkings_gwas_master_2024-workspace-attributes.tsv` to Terra

## Create uBAMs

1. Run Workflow `./wdl/01_FastqToSam_v1.wdl` with configuration `./wdl/01_FastqToSam.1.json` to convert Fastq Files to uBAMs

## Merge uBAMs

1. Download The "sample.tsv" from FireCloud
2.
3. Run this code:
   mv sample.tsv input*data/01_Terra/data_model/
   cd input_data/01_Terra/data_model/
   mkdir sample_metadata
   awk -F "\t" '{file = "./sample_metadata/unaligned_bams*" $12 ".txt"; print $13 > file; close(file)}' samples_ubam_complete.tsv

   # You need to edit the 'participants' table to point to the unaligned bam file path at this point

4. Upload `sample_metadata` directory to bucket
5. Run `./wdl/collect_unaligned_bams_by_participant.1.wdl` with configuration `./wdl/collect_unaligned_bams_by_participant.json`

## Run Preprocess Data

1. Run `./wdl/03_preprocess_data.2.wdl` with configurations `./wdl/preprocess_data_inputs.json` and `./wdl_preprocess_data_outputs.json`

## Run Haplotype Caller

1. Run `./wdl/haplotype_caller.2.wdl` with configuration `./wdl/haplotype_caller.2.json`

This step takes a long time and is probably the most expensive part of the pipeline. Estimate for 300 samples is ~24h

## Run Joint Genotyping

1. Run `./wdl/05_msu_efishlab_joint-discovery-gatk4.4.wdl` with configuration `05_msu_efishlab_joint-discovery-gatk4.4._inputs.json` and `05_msu_efishlab_joint-discovery-gatk4.4_outputs.json`
   This step takes a long time Estimate for 300 samples is ~12-24h

You now have a RAW VCF for downstream analysis!!

## Additional Processing for Mitochondrial Genome

The mitochondrial genome assembly for the new P. kingsleyae genome was based on PacBio Sequencing, which struggles with assembling the circular genome of the mitochondria. The scaffold containing the mtDNA was located in the final assembly at `Scaffold_269__1_contigs__length_38835`. Which repeats the mtGenome several times (fairly standard, based on my reading). In order to reconstruct the mtGenome accurately, the following steps were taken:

1. Find the mtGenome from Assembly

   mitofinder -j [seqid] -a [assembly.fasta] -r [genbank_reference.gb] -o [genetic_code] -p [threads] -m [memory]

2. Prepare mtGenome for Terra Pipeline

Set the origin at the middle of the control region using geneious and exported to fasta `PKINGS_MT_GENOME_mtDNA_contig_reorigin.fasta`

Create Various Indicies

      docker exec gatk ./gatk CreateSequenceDictionary \
      -R /input_data/PKINGS_MT_GENOME/pkings_mito/PKINGS_MT_GENOME_mtDNA_contig_reorigin.fasta \
      -O /input_data/PKINGS_MT_GENOME/pkings_mito/PKINGS_MT_GENOME_mtDNA_contig_reorigin.dict

      samtools faidx PKINGS_MT_GENOME_mtDNA_contig_reorigin.fasta

      bwa index PKINGS_MT_GENOME_mtDNA_contig_reorigin.fasta

Create 'shifted mt genome' for BWA mapping

      docker exec gatk ./gatk ShiftFasta \
         --reference /input_data/PKINGS_MT_GENOME/pkings_mito/PKINGS_MT_GENOME_mtDNA_contig_reorigin.fasta \
         --output /input_data/PKINGS_MT_GENOME/pkings_mito/mito_shift.fasta \
         --shift-back-output /input_data/PKINGS_MT_GENOME/pkings_mito/mito_shift.back_chain \
         --intervals chrM:576-16755 --shift-offset-list 8000 --interval-file-name /input_data/PKINGS_MT_GENOME/pkings_mito/mito \
         --sequence-dictionary /input_data/PKINGS_MT_GENOME/pkings_mito/PKINGS_MT_GENOME_mtDNA_contig_reorigin.dict\
         --tmp-dir .

Create BED file with coordinates for control shifted reference

      chrM	8347	9377

and non-control regions in original:

      chrM	674	16276

Then make interval_list format:

      docker exec gatk ./gatk  BedToIntervalList \
            I=/input_data/PKINGS_MT_GENOME/pkings_mito/control_region_shifted.bed \
            O=/input_data/PKINGS_MT_GENOME/pkings_mito/control_region_shifted.interval_list \
            SD=/input_data/PKINGS_MT_GENOME/pkings_mito/mito_shift.dict

      docker exec gatk ./gatk BedToIntervalList \
            I=/input_data/PKINGS_MT_GENOME/pkings_mito/non_control_region.bed \
            O=/input_data/PKINGS_MT_GENOME/pkings_mito/non_control_region.interval_list \
            SD=/input_data/PKINGS_MT_GENOME/pkings_mito/PKINGS_MT_GENOME_mtDNA_contig_reorigin.dict

Upload relevant files to Terra and process. Note that you have to create a 'blank' blacklisted sites file and index in order to get this to run
