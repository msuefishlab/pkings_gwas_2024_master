## PKINGS Terra Processing
## December 2023

## Copy Data to Google Cloud

All files stored on Amazon S3 need to be transferred to Google Cloud via HPCC.
1. Setup Terra.bio Workspace and obtain bucket for work
2. Setup .env file with the various required parameters
2. Run the following:

  source pkings_gwas.env
  bash code/01_Terra/01_run_upload.sh input_data/01_Terra/2023_file_payload.txt

## Configure Reference Data:

### Create Reference

The run for this reference is the new Paramromyrops kingsleyae *chromosome level* assembly MIC4273_HAP1_MSU618.fa

### Create Interval files

#### setup virtual machines
source pkings_gwas.env
docker run -d -it --name gatk --mount type=bind,source=$root/input_data/00_Reference_Genome/,target=/input_data --mount type=bind,source=$root/input_data/01_Terra/,target=/output_data broadinstitute/gatk

#### prepare files

docker exec gatk cat /input_data/$reference.fai | cut -f1 > $root/input_data/01_Terra/intervals.list

docker exec gatk gatk CreateSequenceDictionary R= /input_data/$reference 

docker exec gatk ./gatk SplitIntervals -R /input_data/$reference -L /output_data/intervals.list -scatter 50 -O /output_data/interval-files -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW

docker exec gatk cat /input_data/$reference.fai | cut -f1,2 | sort -nrk 2 | head -25 | cut -f1 > $root/input_data/01_Terra/chrom_intervals.list
docker exec gatk cat /input_data/$reference.fai | cut -f1,2 | sort -nrk 2 | tail +26 | cut -f1 > $root/input_data/01_Terra/remaining_intervals.list

docker exec gatk ./gatk SplitIntervals -R /input_data/$reference -L /output_data/chrom_intervals.list --scatter-count 50 -O /output_data/chrom-interval-files --subdivision-mode INTERVAL_SUBDIVISION

docker exec gatk ./gatk SplitIntervals -R /input_data/$reference -L /output_data/remaining_intervals.list --scatter-count 50 -O /output_data/remaining-interval-files --subdivision-mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW

cd $root/input_data/01_Terra

ls -1 ./interval-files > ./ifiles.txt

ls -1 ./chrom-interval-files > ./uifiles1.txt
ls -1 ./remaining-interval-files > ./uifiles2.txt


awk -v bucket=$bucket '{ print "gs://"bucket"/interval-files/" $0}' ./ifiles.txt > ./scattered-intervals-file.txt

awk -v bucket=$bucket '{ print "gs://"bucket"/chrom-interval-files/" $0}' ./uifiles1.txt >./unpadded_intervals_file.txt
awk -v bucket=$bucket '{ print "gs://"bucket"/remaining-interval-files/" $0}' ./uifiles2.txt >> ./unpadded_intervals_file.txt

rm ./ifiles.txt
rm ./uifiles1.txt
rm ./uifiles2.txt

#### Upload to Workspace Bucket
1. Transfer `./unpadded_intervals_file.txt`, `./scattered-intervals-file.txt` and the directories `interval-files` and `unpadded-interval-files` to the Google Bucket

    cd input_data/01_Terra
    module purge
    conda activate gcloud

    gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -n ./unpadded_intervals_file.txt gs://${bucket}/
    gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -n ./scattered-intervals-file.txt gs://${bucket}/
    gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -r -n ./interval-files gs://${bucket}/
    gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -r -n ./unpadded-interval-files gs://${bucket}/

## Setup Workspace

1. Upload `.input_data/01_Terra/data_model/fish_data_2023.txt`to Terra
2. Download Airtable `input_data/01_Terra/data_model/Deduplicated Inventory-GWAS_Set.csv`
3. Run
    python3 convert_airtable_to_terra.py
4. Upload `./data_model/samples_2023.tsv` generated by the above python script
5. Generate Reference Index (BWA):
    cd ../00_Reference_Genome/
    bwa index MIC4273_HAP1_MSU618.fa
6. Transfer `input_data/00_Reference_Genome` to Terra
    conda activate gcloud
    source pkings_gwas.env
    gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -n -r ./input_data/00_Reference_Genome gs://${bucket}/reference

5. Upload `./data_model/pkings_gwas_master_2024-workspace-attributes.tsv` to Terra

## Create uBAMs
1. Run Workflow `./wdl/01_FastqToSam_v1.wdl` with configuration `./wdl/01_FastqToSam.1.json` to convert Fastq Files to uBAMs

## Merge uBAMs
1. Download The "Samples.TSV" from FireCloud
2. Run this code:
    cd input_data/01_Terra/data_model/
    mkdir sample_metadata
    awk -F "\t" '{print $13 >> ("./sample_metadata/unaligned_bams_" $12 ".txt")} ; close("./sample_metadata/unaligned_bams_" $12".txt")' samples_ubam_complete.tsv
3. Upload `sample_metadata` directory to bucket
4. Run `./wdl/collect_unaligned_bams_by_participant.1.wdl` with configuration `./wdl/collect_unaligned_bams_by_participant.json`

## Run Preprocess Data
1. Run `./wdl/03_preprocess_data.2.wdl` with configurations `./wdl/preprocess_data_inputs.json` and `./wdl_preprocess_data_outputs.json`

## Run Haplotype Caller
1. Run `./wdl/haplotype_caller.2.wdl` with configuration `./wdl/haplotype_caller.2.json`

This step takes a long time and is probably the most expensive part of the pipeline.  Estimate for 300 samples is ~24h

## Run Joint Genotyping
1. Run `./wdl/05_msu_efishlab_joint-discovery-gatk4.4.wdl` with configuration `05_msu_efishlab_joint-discovery-gatk4.4._inputs.json` and `05_msu_efishlab_joint-discovery-gatk4.4_outputs.json`
This step takes a long time Estimate for 300 samples is ~12-24h

You now have a RAW VCF for downstream analysis!!
